# ReaL Reproduction

This repository contains the code and artifacts needed to reproduce the results of **Project ReaL** on EDA automation tasks.

---

## Overview

This repository helps the viewer to reproduce our results. It contains the outputs generated by our 7B model across 27 distinct tasks.

---

## Inputs

* **Prompt file:** `prompt.json`
  Contains the 27 task specifications and corresponding prompt instructions.

---

## Outputs

* **`exported_scripts/`**
  A folder containing the model’s generated OpenROAD scripts.
  All scripts have been verified to run in the OpenROAD Docker environment (see ECE240 Lab 6).

---

## Notices

* You may see warnings when running OpenROAD scripts. These arise because we do not supply full chip‑ and process‑design files (our focus is on EDA functionality, not physical design).
* Our training corpus similarly omits full context; it emphasizes script functionality over complete design flows.

---

## Evaluation

1. **Functionality**

   * Tool: `detectoreval.py`
   * Result: 21/27 tasks passed → **77.78 % accuracy**
   * Detailed results are in `detect_results.json`.

2. **Quality**

   * Tool: `llm_judge_ability_test.py`
   * Result: 22/27 tasks passed → **81.48 % accuracy**
   * Raw judgments are in `llm_judge_result.txt`.

---

## Reproduction

To reproduce our results:

1. **Clone the repository**

   ```bash
   git clone https://github.com/Colin0vO/ReaL_reproduction.git
   cd ReaL_reproduction
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Configure your API key**
   Create a `.env` file in the project root with the following content:

   ```ini
    OPENAI_API_KEY="Your API key"
    
    # Set to false to disable anonymized telemetry
    ANONYMIZED_TELEMETRY=true
    
    # LogLevel: Set to debug to enable verbose logging, set to result to get results only. Available: result | debug | info
    BROWSER_USE_LOGGING_LEVEL=info
   ```

4. **Run the full evaluation**

   ```bash
   bash run.bash
   ```

   This script will:

   * Execute `detectoreval.py` (outputs `detect_results.json`)
   * Execute `llm_judge_ability_test.py` (outputs `llm_judge_result.txt`)

After completion, inspect ur terminals so that u can see accuracy.

A sample output is:
root@rfa-deploy-66df545575-nrzj9:/mnt/data/base_clone/ReaL_reproduction# bash run.bash
Converted 27 scripts → responses.json
Running detectors: 100%|████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 236.20it/s]

{
  "pass_count": 21,
  "total": 27,
  "pass_rate": 77.78,
  "details": [
    {
      …
    }
  ]
}
Wrote 27 entries to paired.json
Total checks:       27
YES count:          22
NO count:           5
Percentage of YES:  81.48%


---

## License & Contributions

Feel free to contact me at zhw106@ucsd.edu if u meet difficulties in setting up.
